<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sociedade on Paula Bicca</title><link>https://paulabicca.github.io/pt-br/categories/sociedade/</link><description>Recent content in Sociedade on Paula Bicca</description><generator>Hugo -- gohugo.io</generator><language>pt-br</language><lastBuildDate>Wed, 20 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://paulabicca.github.io/pt-br/categories/sociedade/index.xml" rel="self" type="application/rss+xml"/><item><title>Negros e Algoritmos: A Urgente Necessidade de Diversidade na Tecnologia</title><link>https://paulabicca.github.io/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://paulabicca.github.io/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/</guid><description>&lt;p>Hoje é um dia especial, um dia de reconhecimento. Como mulher negra, moradora de Porto Alegre, RS, este é o primeiro ano em que o Dia da Consciência Negra é feriado aqui. Para além de ser uma simples data no calendário, é uma homenagem aos que vieram antes de nós, uma memória de lutas e resistências. Celebramos Zumbi dos Palmares, líder do Quilombo dos Palmares, e toda a trajetória de resistência dos afrodescendentes contra a escravidão e o racismo que persistem até hoje.&lt;/p>
&lt;p>Eu confesso que, até o dia 20 chegar, eu não tinha conseguido pensado em nada para essa data tão significativa. Mas, de repente, a ideia surgiu, sem aviso, como um estalo.&lt;/p>
&lt;p>Anos atrás, durante uma palestra no meu estágio, ouvi pela primeira vez um assunto que me fez refletir profundamente sobre o impacto dos algoritmos nas tecnologias que usamos diariamente. A palestrante mencionou um erro importante ocorrido com o Google Photos, um incidente que levantou questões sobre a ética e a diversidade no desenvolvimento de inteligência artificial (IA). O Google Fotos foi lançado em 2015 e é um serviço de compartilhamento e armazenamento de fotos desenvolvido pelo Google. É possível rotular imagens, identificando pessoas, objetos e lugares automaticamente.
No entanto, poucos meses após o lançamento, um erro veio à tona, destacando uma falha na implementação de IA que, até hoje, continua sendo uma ilustração evidente de como os algoritmos podem herdar vieses e preconceitos.&lt;/p>
&lt;h2 id="a-falta-de-diversidade-nos-dados-de-treinamento">A Falta de Diversidade nos Dados de Treinamento
&lt;/h2>&lt;p>O problema começou quando Jacky Alciné, um desenvolvedor de software de Nova York, percebeu que o Google Photos havia rotulado suas fotos, juntamente com as de seu amigo, ambos negros, como “gorilas”. Este erro não foi apenas um deslize técnico, mas um reflexo doloroso de séculos de racismo, demonstrando como a IA pode, inadvertidamente, reproduzir preconceitos já presentes na sociedade.
Após a denúncia de Alciné no Twitter, o Google se desculpou publicamente, alegando estar “chocado e genuinamente arrependido” pelo erro. A empresa prometeu corrigir o problema, afirmando que o rótulo “gorila” não seria mais utilizado em nenhuma imagem e que tomariam medidas para melhorar tanto a parte linguística quanto o reconhecimento de imagens, com ênfase na melhoria do reconhecimento de rostos de pele escura. No entanto, o incidente trouxe à tona uma realidade incômoda: os algoritmos de IA herdam os preconceitos presentes nas bases de dados com as quais são treinados e, no caso do Google Photos, faltava uma diversidade suficiente de imagens de pessoas negras nos dados usados para o treinamento.&lt;/p>
&lt;h2 id="o-que-mudou-desde-então">O Que Mudou Desde Então?
&lt;/h2>&lt;p>O Google afirmou na época que o problema seria consertado, mas será que realmente foi? Uma reportagem de maio de 2023 do &lt;em>&lt;a class="link" href="https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html#:~:text=Yet%20Google%2C%20whose%20Android%20software,a%20person%20as%20an%20animal" target="_blank" rel="noopener"
>The New York Times&lt;/a>&lt;/em> revelou que, oito anos após o incidente, o Google ainda não resolveu o problema de forma satisfatória. Em vez de melhorar a tecnologia de reconhecimento de imagens, o Google simplesmente optou por desativar a função de identificação de gorilas. Isso, claro, limitou a capacidade do aplicativo de identificar primatas de forma geral, mas evitou o risco de cometer o mesmo erro novamente.
O New York Times investigou sua própria coleção de imagens e descobriu que, ao pesquisar por “gorilas”, o Google Fotos não conseguiu identificar nenhum primata, mesmo quando algumas imagens contêm gorilas. Além disso, a Apple, ao ser questionada sobre um problema similar com o Apple Photos, revelou que tomou uma abordagem semelhante, desativando a capacidade de identificar gorilas nas imagens, por preocupações de que o sistema pudesse erroneamente rotular uma pessoa como um animal.&lt;/p>
&lt;h2 id="por-que-isso-importa">Por que isso importa?
&lt;/h2>&lt;p>O que essas falhas revelam é a dificuldade das grandes empresas de tecnologia em lidar com o preconceito e a falta de diversidade em seus algoritmos de IA. A solução adotada pelas duas gigantes, Google e Apple, de simplesmente desativar a função de identificação de gorilas, reflete uma solução paliativa, mas levanta questões ainda mais amplas sobre o papel da diversidade na criação de tecnologias justas e precisas. A decisão de limitar a funcionalidade, em vez de melhorar o sistema, ilustra como as empresas de tecnologia muitas vezes priorizam a redução de riscos em detrimento de soluções mais eficazes.
Esse episódio mostra claramente que os algoritmos de IA não são imparciais. Eles são criados por seres humanos e treinados com dados que, muitas vezes, carecem de diversidade e precisão. Quando isso acontece, as consequências podem ser graves: um simples erro de rotulagem pode reforçar preconceitos sociais existentes e perpetuar estigmas prejudiciais. A falta de diversidade nos dados de treinamento — seja em termos de etnia, gênero ou qualquer outra característica — pode resultar em sistemas que não funcionam corretamente para todos, afetando especialmente os grupos historicamente marginalizados.&lt;/p>
&lt;h2 id="a-diversidade-na-tecnologia-um-desafio-a-ser-superado">A Diversidade na Tecnologia: Um Desafio a Ser Superado
&lt;/h2>&lt;p>Em um mundo cada vez mais digital e dependente de IA, a diversidade na tecnologia não é apenas uma questão ética, mas uma necessidade prática. Sem representatividade nos dados com os quais treinamos nossos algoritmos, corremos o risco de criar sistemas que excluem ou prejudicam determinados grupos. A questão vai além do Google e da Apple; ela afeta todos os desenvolvedores e profissionais de tecnologia. Precisamos garantir que nossas bases de dados de treinamento sejam amplamente representativas, não apenas em termos de etnia, mas também de gênero, idade, capacidade e muito mais. Somente assim poderemos criar soluções de IA que realmente atendam às necessidades de todos, sem reproduzir os preconceitos do passado.&lt;/p>
&lt;h2 id="o-que-podemos-fazer-para-avançar">O Que Podemos Fazer Para Avançar?
&lt;/h2>&lt;p>A tecnologia de reconhecimento de imagens tem avançado imensamente, mas a questão dos preconceitos e vieses permanece. A falta de diversidade nos dados de treinamento e nas equipes de desenvolvimento é um obstáculo que todos nós, como profissionais da área de tecnologia, precisamos superar. Como podemos garantir que os sistemas que criamos não apenas funcionem, mas também sejam justos e inclusivos? A resposta está na promoção de uma maior diversidade e inclusão em todos os aspectos do desenvolvimento tecnológico, desde a coleta de dados até a criação dos algoritmos.&lt;/p>
&lt;hr>
&lt;p>Como desenvolvedores, temos um papel crucial em moldar o futuro da tecnologia. Vamos refletir sobre como estamos criando os sistemas de IA com os quais trabalhamos. Estamos garantindo que eles sejam inclusivos e representem todos de forma justa? Se não, como podemos começar a fazer mudanças significativas? Compartilhe suas ideias e experiências, e juntos podemos trabalhar para construir um futuro mais inclusivo na tecnologia.&lt;/p>
&lt;p>&lt;em>As informações apresentadas neste post foram obtidas no ekathimerini.com, que fez referência ao artigo original do The New York Times.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Referências&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;em>BBC. (2015). Google apologises for Photos app&amp;rsquo;s racist blunder. Disponível &lt;a class="link" href="https://www.bbc.com/news/technology-33347866" target="_blank" rel="noopener"
>aqui&lt;/a>&lt;/em>&lt;/li>
&lt;li>&lt;em>Ekathimerini. (2023). Google’s photo app still can’t find gorillas. And neither can Apple’s. Disponível &lt;a class="link" href="https://www.ekathimerini.com/nytimes/1212118/googles-photo-app-still-cant-find-gorillas-and-neither-can-apples/" target="_blank" rel="noopener"
>aqui&lt;/a>&lt;/em>&lt;/li>
&lt;li>&lt;em>The New York Times. (2023). Google’s Photo App Still Can’t Find Gorillas. And Neither Can Apple’s. Disponível &lt;a class="link" href="https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html#:~:text=Yet%20Google%2C%20whose%20Android%20software,a%20person%20as%20an%20animal" target="_blank" rel="noopener"
>aqui&lt;/a>&lt;/em>&lt;/li>
&lt;/ul></description></item><item><title>Magreza como tendência e o papel do algoritmo: até onde vai a responsabilidade?</title><link>https://paulabicca.github.io/pt-br/p/magreza-como-descricao-e-o-papel-do-algoritmo/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://paulabicca.github.io/pt-br/p/magreza-como-descricao-e-o-papel-do-algoritmo/</guid><description>&lt;p>A primeira versão dessa publicação foi o que me motivou a criar este blog. Você pode conferir uma versão reduzida dela &lt;a class="link" href="https://www.linkedin.com/posts/paulabicca93_tecnologia-algoritmo-mulheres-activity-7262461647829532672-eOik?utm_source=share&amp;amp;utm_medium=member_desktop" target="_blank" rel="noopener"
>aqui&lt;/a>. Quando publiquei o primeiro post no blog, decidi que este seria o segundo, especialmente porque, nesse meio tempo, assisti a um filme que se conecta bastante com o tema.&lt;/p>
&lt;p>Algumas semanas atrás, assisti ao filme &lt;strong>A Morte Lhe Cai&lt;/strong> , que conta a história de duas mulheres dispostas a fazer qualquer coisa para recuperar a beleza e juventude que um dia tiveram. Recentemente, vi &lt;strong>A Substância&lt;/strong>, que retrata a trajetória de uma atriz cuja carreira desmorona com a chegada da velhice. Desesperada, ela toma uma substância misteriosa que traz uma nova versão de dela mesma.&lt;/p>
&lt;p>Esses filmes me fizeram lembrar de uma trend que tem circulado no TikTok. Nela, meninas, adolescentes e mulheres compartilham práticas extremas, como passar o dia apenas comendo gelatina ou até celebrar a dor de barriga, porque isso as ajudaria a perder peso.&lt;/p>
&lt;p>&lt;strong>O que esses 3 pontos têm em comum?&lt;/strong>&lt;/p>
&lt;p>A eterna busca pela juventude e magreza. Nos filmes e na trend, fica evidente que muitas mulheres estão dispostas a fazer quase qualquer coisa para parecerem mais jovens, mais magras e, por consequência, &amp;ldquo;mais bonitas e desejáveis&amp;rdquo;.&lt;/p>
&lt;p>Essa reflexão me leva a questionar: até onde a culpa é do algoritmo? Ele deve ser responsabilizado? E mais importante, ele pode ser responsabilizado?&lt;/p>
&lt;p>Segundo as diretrizes da plataforma, a idade mínima para uso do TikTok é 13 anos (ou 14 na Coreia do Sul, Indonésia e Quebec). É comum encontrar crianças bem mais novas por lá, expostas a conteúdos e padrões estéticos muitas vezes prejudiciais. E o impacto desse conteúdo não se restringe apenas às crianças; afeta também adolescentes e mulheres adultas que sentem uma pressão constante para atingir o padrão de magreza e beleza exaltado nas redes sociais.&lt;/p>
&lt;p>Apesar das diretrizes de conteúdo específicas do TikTok proibirem a exibição de práticas prejudiciais relacionadas ao controle de peso (como dietas de baixas calorias e uso de medicamentos para emagrecimento), esses conteúdos se disseminam de forma ampla e descontrolada. Nos Estados Unidos, por exemplo, há uma versão do TikTok para menores de 13 anos, que inclui proteções adicionais, como restrições de interação e avaliações de conteúdo apropriado para essa faixa etária. No entanto, essa proteção não impede que conteúdos nocivos alcancem outras faixas etárias, inclusive adolescentes e adultos.&lt;/p>
&lt;h2 id="a-responsabilidade-do-algoritmo">A responsabilidade do algoritmo
&lt;/h2>&lt;p>O TikTok, como muitas outras redes sociais, utiliza algoritmos de recomendação que são projetados para manter os usuários engajados, entregando conteúdos com base nos interesses e comportamentos de navegação de cada pessoa. Em outras palavras, quanto mais você interage com conteúdos sobre beleza e dietas, mais o algoritmo entrega publicações semelhantes.&lt;/p>
&lt;h2 id="como-podemos-melhorar-esse-cenário">Como podemos melhorar esse cenário?
&lt;/h2>&lt;p>É importante que as redes sociais invistam em algoritmos mais responsáveis, que filtrem conteúdos prejudiciais à saúde mental e física dos usuários, especialmente dos mais jovens. Além disso, educar os usuários para reconhecerem e questionarem esses padrões também é essencial. Parcerias com organizações de saúde mental e órgãos reguladores para reforçar a segurança e saúde digital podem ser passos importantes.&lt;/p>
&lt;p>Como sociedade, podemos promover uma cultura de aceitação e bem-estar, criando conteúdo positivo e pressionando as redes para uma moderação mais rigorosa e responsável. A sociedade e as plataformas devem trabalhar em conjunto para reduzir essa pressão estética, priorizando o bem-estar e a autoaceitação em vez de padrões inalcançáveis.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>E você, o que acha desse tema?&lt;/strong> Já parou para pensar no impacto do algoritmo nas nossas vidas? Deixe um comentário com sua opinião ou compartilhe alguma experiência que tenha vivido sobre isso. Vamos conversar e pensar juntos em como podemos criar uma internet mais saudável e inclusiva!&lt;/p></description></item></channel></rss>