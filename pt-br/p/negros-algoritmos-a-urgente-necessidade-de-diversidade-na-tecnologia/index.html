<!doctype html><html lang=pt-br dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="No Dia da Consciência Negra, veja como a falta de diversidade nos algoritmos de IA pode reforçar preconceitos"><title>Negros e Algoritmos: A Urgente Necessidade de Diversidade na Tecnologia</title>
<link rel=canonical href=https://paulabicca.github.io/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Negros e Algoritmos: A Urgente Necessidade de Diversidade na Tecnologia"><meta property='og:description' content="No Dia da Consciência Negra, veja como a falta de diversidade nos algoritmos de IA pode reforçar preconceitos"><meta property='og:url' content='https://paulabicca.github.io/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/'><meta property='og:site_name' content='Paula Bicca'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='algoritmo'><meta property='article:tag' content='Tecnologia Justa'><meta property='article:tag' content='Racismo Na Tecnologia'><meta property='article:tag' content='Inteligência Artificial'><meta property='article:published_time' content='2024-11-20T00:00:00+00:00'><meta property='article:modified_time' content='2024-11-20T00:00:00+00:00'><meta name=twitter:title content="Negros e Algoritmos: A Urgente Necessidade de Diversidade na Tecnologia"><meta name=twitter:description content="No Dia da Consciência Negra, veja como a falta de diversidade nos algoritmos de IA pode reforçar preconceitos"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Alternar Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/pt-br/><img src=/img/avatar_hu15431975163954554691.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>✨</span></figure><div class=site-meta><h1 class=site-name><a href=/pt-br>Paula Bicca</a></h1><h2 class=site-description>Desenvolvedora Front-End que ama novidades, reflexões e acessibilidade.</h2></div></header><ol class=menu-social><li><a href=mailto:paula.bicca@gmail.com target=_blank title=Email rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-mail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="M3 7l9 6 9-6"/></svg></a></li><li><a href=https://github.com/paulabicca target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/paulabicca93/ target=_blank title=LinkedIn rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-brand-linkedin"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 11v5"/><path d="M8 8v.01"/><path d="M12 16v-5"/><path d="M16 16v-3a2 2 0 10-4 0"/><path d="M3 7a4 4 0 014-4h10a4 4 0 014 4v10a4 4 0 01-4 4H7a4 4 0 01-4-4z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/pt-br/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Início</span></a></li><li><a href=/pt-br/about-me/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-user"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 7a4 4 0 108 0A4 4 0 008 7"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>Sobre mim</span></a></li><li><a href=/pt-br/articles/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-article"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 4m0 2a2 2 0 012-2h14a2 2 0 012 2v12a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><path d="M7 8h10"/><path d="M7 12h10"/><path d="M7 16h10"/></svg>
<span>Publicações</span></a></li><li><a href=/pt-br/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Buscar</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://paulabicca.github.io/pt-br/ selected>Português</option><option value=https://paulabicca.github.io/en/>English</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Modo Escuro</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Índice</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#a-falta-de-diversidade-nos-dados-de-treinamento>A Falta de Diversidade nos Dados de Treinamento</a></li><li><a href=#o-que-mudou-desde-então>O Que Mudou Desde Então?</a></li><li><a href=#por-que-isso-importa>Por que isso importa?</a></li><li><a href=#a-diversidade-na-tecnologia-um-desafio-a-ser-superado>A Diversidade na Tecnologia: Um Desafio a Ser Superado</a></li><li><a href=#o-que-podemos-fazer-para-avançar>O Que Podemos Fazer Para Avançar?</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/pt-br/categories/diversidade/>Diversidade
</a><a href=/pt-br/categories/sociedade/>Sociedade</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/>Negros e Algoritmos: A Urgente Necessidade de Diversidade na Tecnologia</a></h2><h3 class=article-subtitle>No Dia da Consciência Negra, veja como a falta de diversidade nos algoritmos de IA pode reforçar preconceitos</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>nov. 20, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>6 minutos de leitura</time></div></footer></div></header><section class=article-content><p>Hoje é um dia especial, um dia de reconhecimento. Como mulher negra, moradora de Porto Alegre, RS, este é o primeiro ano em que o Dia da Consciência Negra é feriado aqui. Para além de ser uma simples data no calendário, é uma homenagem aos que vieram antes de nós, uma memória de lutas e resistências. Celebramos Zumbi dos Palmares, líder do Quilombo dos Palmares, e toda a trajetória de resistência dos afrodescendentes contra a escravidão e o racismo que persistem até hoje.</p><p>Eu confesso que, até o dia 20 chegar, eu não tinha conseguido pensado em nada para essa data tão significativa. Mas, de repente, a ideia surgiu, sem aviso, como um estalo.</p><p>Anos atrás, durante uma palestra no meu estágio, ouvi pela primeira vez um assunto que me fez refletir profundamente sobre o impacto dos algoritmos nas tecnologias que usamos diariamente. A palestrante mencionou um erro importante ocorrido com o Google Photos, um incidente que levantou questões sobre a ética e a diversidade no desenvolvimento de inteligência artificial (IA). O Google Fotos foi lançado em 2015 e é um serviço de compartilhamento e armazenamento de fotos desenvolvido pelo Google. É possível rotular imagens, identificando pessoas, objetos e lugares automaticamente.
No entanto, poucos meses após o lançamento, um erro veio à tona, destacando uma falha na implementação de IA que, até hoje, continua sendo uma ilustração evidente de como os algoritmos podem herdar vieses e preconceitos.</p><h2 id=a-falta-de-diversidade-nos-dados-de-treinamento>A Falta de Diversidade nos Dados de Treinamento</h2><p>O problema começou quando Jacky Alciné, um desenvolvedor de software de Nova York, percebeu que o Google Photos havia rotulado suas fotos, juntamente com as de seu amigo, ambos negros, como “gorilas”. Este erro não foi apenas um deslize técnico, mas um reflexo doloroso de séculos de racismo, demonstrando como a IA pode, inadvertidamente, reproduzir preconceitos já presentes na sociedade.
Após a denúncia de Alciné no Twitter, o Google se desculpou publicamente, alegando estar “chocado e genuinamente arrependido” pelo erro. A empresa prometeu corrigir o problema, afirmando que o rótulo “gorila” não seria mais utilizado em nenhuma imagem e que tomariam medidas para melhorar tanto a parte linguística quanto o reconhecimento de imagens, com ênfase na melhoria do reconhecimento de rostos de pele escura. No entanto, o incidente trouxe à tona uma realidade incômoda: os algoritmos de IA herdam os preconceitos presentes nas bases de dados com as quais são treinados e, no caso do Google Photos, faltava uma diversidade suficiente de imagens de pessoas negras nos dados usados para o treinamento.</p><h2 id=o-que-mudou-desde-então>O Que Mudou Desde Então?</h2><p>O Google afirmou na época que o problema seria consertado, mas será que realmente foi? Uma reportagem de maio de 2023 do <em><a class=link href="https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html#:~:text=Yet%20Google%2C%20whose%20Android%20software,a%20person%20as%20an%20animal" target=_blank rel=noopener>The New York Times</a></em> revelou que, oito anos após o incidente, o Google ainda não resolveu o problema de forma satisfatória. Em vez de melhorar a tecnologia de reconhecimento de imagens, o Google simplesmente optou por desativar a função de identificação de gorilas. Isso, claro, limitou a capacidade do aplicativo de identificar primatas de forma geral, mas evitou o risco de cometer o mesmo erro novamente.
O New York Times investigou sua própria coleção de imagens e descobriu que, ao pesquisar por “gorilas”, o Google Fotos não conseguiu identificar nenhum primata, mesmo quando algumas imagens contêm gorilas. Além disso, a Apple, ao ser questionada sobre um problema similar com o Apple Photos, revelou que tomou uma abordagem semelhante, desativando a capacidade de identificar gorilas nas imagens, por preocupações de que o sistema pudesse erroneamente rotular uma pessoa como um animal.</p><h2 id=por-que-isso-importa>Por que isso importa?</h2><p>O que essas falhas revelam é a dificuldade das grandes empresas de tecnologia em lidar com o preconceito e a falta de diversidade em seus algoritmos de IA. A solução adotada pelas duas gigantes, Google e Apple, de simplesmente desativar a função de identificação de gorilas, reflete uma solução paliativa, mas levanta questões ainda mais amplas sobre o papel da diversidade na criação de tecnologias justas e precisas. A decisão de limitar a funcionalidade, em vez de melhorar o sistema, ilustra como as empresas de tecnologia muitas vezes priorizam a redução de riscos em detrimento de soluções mais eficazes.
Esse episódio mostra claramente que os algoritmos de IA não são imparciais. Eles são criados por seres humanos e treinados com dados que, muitas vezes, carecem de diversidade e precisão. Quando isso acontece, as consequências podem ser graves: um simples erro de rotulagem pode reforçar preconceitos sociais existentes e perpetuar estigmas prejudiciais. A falta de diversidade nos dados de treinamento — seja em termos de etnia, gênero ou qualquer outra característica — pode resultar em sistemas que não funcionam corretamente para todos, afetando especialmente os grupos historicamente marginalizados.</p><h2 id=a-diversidade-na-tecnologia-um-desafio-a-ser-superado>A Diversidade na Tecnologia: Um Desafio a Ser Superado</h2><p>Em um mundo cada vez mais digital e dependente de IA, a diversidade na tecnologia não é apenas uma questão ética, mas uma necessidade prática. Sem representatividade nos dados com os quais treinamos nossos algoritmos, corremos o risco de criar sistemas que excluem ou prejudicam determinados grupos. A questão vai além do Google e da Apple; ela afeta todos os desenvolvedores e profissionais de tecnologia. Precisamos garantir que nossas bases de dados de treinamento sejam amplamente representativas, não apenas em termos de etnia, mas também de gênero, idade, capacidade e muito mais. Somente assim poderemos criar soluções de IA que realmente atendam às necessidades de todos, sem reproduzir os preconceitos do passado.</p><h2 id=o-que-podemos-fazer-para-avançar>O Que Podemos Fazer Para Avançar?</h2><p>A tecnologia de reconhecimento de imagens tem avançado imensamente, mas a questão dos preconceitos e vieses permanece. A falta de diversidade nos dados de treinamento e nas equipes de desenvolvimento é um obstáculo que todos nós, como profissionais da área de tecnologia, precisamos superar. Como podemos garantir que os sistemas que criamos não apenas funcionem, mas também sejam justos e inclusivos? A resposta está na promoção de uma maior diversidade e inclusão em todos os aspectos do desenvolvimento tecnológico, desde a coleta de dados até a criação dos algoritmos.</p><hr><p>Como desenvolvedores, temos um papel crucial em moldar o futuro da tecnologia. Vamos refletir sobre como estamos criando os sistemas de IA com os quais trabalhamos. Estamos garantindo que eles sejam inclusivos e representem todos de forma justa? Se não, como podemos começar a fazer mudanças significativas? Compartilhe suas ideias e experiências, e juntos podemos trabalhar para construir um futuro mais inclusivo na tecnologia.</p><p><em>As informações apresentadas neste post foram obtidas no ekathimerini.com, que fez referência ao artigo original do The New York Times.</em></p><p><strong>Referências</strong></p><ul><li><em>BBC. (2015). Google apologises for Photos app&rsquo;s racist blunder. Disponível <a class=link href=https://www.bbc.com/news/technology-33347866 target=_blank rel=noopener>aqui</a></em></li><li><em>Ekathimerini. (2023). Google’s photo app still can’t find gorillas. And neither can Apple’s. Disponível <a class=link href=https://www.ekathimerini.com/nytimes/1212118/googles-photo-app-still-cant-find-gorillas-and-neither-can-apples/ target=_blank rel=noopener>aqui</a></em></li><li><em>The New York Times. (2023). Google’s Photo App Still Can’t Find Gorillas. And Neither Can Apple’s. Disponível <a class=link href="https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html#:~:text=Yet%20Google%2C%20whose%20Android%20software,a%20person%20as%20an%20animal" target=_blank rel=noopener>aqui</a></em></li></ul></section><footer class=article-footer><section class=article-tags><a href=/pt-br/tags/algoritmo/>Algoritmo</a>
<a href=/pt-br/tags/tecnologia-justa/>Tecnologia Justa</a>
<a href=/pt-br/tags/racismo-na-tecnologia/>Racismo Na Tecnologia</a>
<a href=/pt-br/tags/intelig%C3%AAncia-artificial/>Inteligência Artificial</a></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>Última atualização em nov. 20, 2024</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>Conteúdo relacionado</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/pt-br/p/magreza-como-descricao-e-o-papel-do-algoritmo/><div class=article-details><h2 class=article-title>Magreza como tendência e o papel do algoritmo: até onde vai a responsabilidade?</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=paulabicca/paulabicca.github.io issue-term=pathname label=Comentários crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 Paula Bicca</section><section class=powerby>Criado com <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Tema <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> desenvolvido por <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>