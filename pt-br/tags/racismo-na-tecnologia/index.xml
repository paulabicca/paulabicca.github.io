<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Racismo Na Tecnologia on Paula Bicca</title><link>https://paulabicca.github.io/pt-br/tags/racismo-na-tecnologia/</link><description>Recent content in Racismo Na Tecnologia on Paula Bicca</description><generator>Hugo -- gohugo.io</generator><language>pt-br</language><lastBuildDate>Wed, 20 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://paulabicca.github.io/pt-br/tags/racismo-na-tecnologia/index.xml" rel="self" type="application/rss+xml"/><item><title>Negros e Algoritmos: A Urgente Necessidade de Diversidade na Tecnologia</title><link>https://paulabicca.github.io/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://paulabicca.github.io/pt-br/p/negros-algoritmos-a-urgente-necessidade-de-diversidade-na-tecnologia/</guid><description>&lt;p>Hoje é um dia especial, um dia de reconhecimento. Como mulher negra, moradora de Porto Alegre, RS, este é o primeiro ano em que o Dia da Consciência Negra é feriado aqui. Para além de ser uma simples data no calendário, é uma homenagem aos que vieram antes de nós, uma memória de lutas e resistências. Celebramos Zumbi dos Palmares, líder do Quilombo dos Palmares, e toda a trajetória de resistência dos afrodescendentes contra a escravidão e o racismo que persistem até hoje.&lt;/p>
&lt;p>Eu confesso que, até o dia 20 chegar, eu não tinha conseguido pensado em nada para essa data tão significativa. Mas, de repente, a ideia surgiu, sem aviso, como um estalo.&lt;/p>
&lt;p>Anos atrás, durante uma palestra no meu estágio, ouvi pela primeira vez um assunto que me fez refletir profundamente sobre o impacto dos algoritmos nas tecnologias que usamos diariamente. A palestrante mencionou um erro importante ocorrido com o Google Photos, um incidente que levantou questões sobre a ética e a diversidade no desenvolvimento de inteligência artificial (IA). O Google Fotos foi lançado em 2015 e é um serviço de compartilhamento e armazenamento de fotos desenvolvido pelo Google. É possível rotular imagens, identificando pessoas, objetos e lugares automaticamente.
No entanto, poucos meses após o lançamento, um erro veio à tona, destacando uma falha na implementação de IA que, até hoje, continua sendo uma ilustração evidente de como os algoritmos podem herdar vieses e preconceitos.&lt;/p>
&lt;h2 id="a-falta-de-diversidade-nos-dados-de-treinamento">A Falta de Diversidade nos Dados de Treinamento
&lt;/h2>&lt;p>O problema começou quando Jacky Alciné, um desenvolvedor de software de Nova York, percebeu que o Google Photos havia rotulado suas fotos, juntamente com as de seu amigo, ambos negros, como “gorilas”. Este erro não foi apenas um deslize técnico, mas um reflexo doloroso de séculos de racismo, demonstrando como a IA pode, inadvertidamente, reproduzir preconceitos já presentes na sociedade.
Após a denúncia de Alciné no Twitter, o Google se desculpou publicamente, alegando estar “chocado e genuinamente arrependido” pelo erro. A empresa prometeu corrigir o problema, afirmando que o rótulo “gorila” não seria mais utilizado em nenhuma imagem e que tomariam medidas para melhorar tanto a parte linguística quanto o reconhecimento de imagens, com ênfase na melhoria do reconhecimento de rostos de pele escura. No entanto, o incidente trouxe à tona uma realidade incômoda: os algoritmos de IA herdam os preconceitos presentes nas bases de dados com as quais são treinados e, no caso do Google Photos, faltava uma diversidade suficiente de imagens de pessoas negras nos dados usados para o treinamento.&lt;/p>
&lt;h2 id="o-que-mudou-desde-então">O Que Mudou Desde Então?
&lt;/h2>&lt;p>O Google afirmou na época que o problema seria consertado, mas será que realmente foi? Uma reportagem de maio de 2023 do &lt;em>&lt;a class="link" href="https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html#:~:text=Yet%20Google%2C%20whose%20Android%20software,a%20person%20as%20an%20animal" target="_blank" rel="noopener"
>The New York Times&lt;/a>&lt;/em> revelou que, oito anos após o incidente, o Google ainda não resolveu o problema de forma satisfatória. Em vez de melhorar a tecnologia de reconhecimento de imagens, o Google simplesmente optou por desativar a função de identificação de gorilas. Isso, claro, limitou a capacidade do aplicativo de identificar primatas de forma geral, mas evitou o risco de cometer o mesmo erro novamente.
O New York Times investigou sua própria coleção de imagens e descobriu que, ao pesquisar por “gorilas”, o Google Fotos não conseguiu identificar nenhum primata, mesmo quando algumas imagens contêm gorilas. Além disso, a Apple, ao ser questionada sobre um problema similar com o Apple Photos, revelou que tomou uma abordagem semelhante, desativando a capacidade de identificar gorilas nas imagens, por preocupações de que o sistema pudesse erroneamente rotular uma pessoa como um animal.&lt;/p>
&lt;h2 id="por-que-isso-importa">Por que isso importa?
&lt;/h2>&lt;p>O que essas falhas revelam é a dificuldade das grandes empresas de tecnologia em lidar com o preconceito e a falta de diversidade em seus algoritmos de IA. A solução adotada pelas duas gigantes, Google e Apple, de simplesmente desativar a função de identificação de gorilas, reflete uma solução paliativa, mas levanta questões ainda mais amplas sobre o papel da diversidade na criação de tecnologias justas e precisas. A decisão de limitar a funcionalidade, em vez de melhorar o sistema, ilustra como as empresas de tecnologia muitas vezes priorizam a redução de riscos em detrimento de soluções mais eficazes.
Esse episódio mostra claramente que os algoritmos de IA não são imparciais. Eles são criados por seres humanos e treinados com dados que, muitas vezes, carecem de diversidade e precisão. Quando isso acontece, as consequências podem ser graves: um simples erro de rotulagem pode reforçar preconceitos sociais existentes e perpetuar estigmas prejudiciais. A falta de diversidade nos dados de treinamento — seja em termos de etnia, gênero ou qualquer outra característica — pode resultar em sistemas que não funcionam corretamente para todos, afetando especialmente os grupos historicamente marginalizados.&lt;/p>
&lt;h2 id="a-diversidade-na-tecnologia-um-desafio-a-ser-superado">A Diversidade na Tecnologia: Um Desafio a Ser Superado
&lt;/h2>&lt;p>Em um mundo cada vez mais digital e dependente de IA, a diversidade na tecnologia não é apenas uma questão ética, mas uma necessidade prática. Sem representatividade nos dados com os quais treinamos nossos algoritmos, corremos o risco de criar sistemas que excluem ou prejudicam determinados grupos. A questão vai além do Google e da Apple; ela afeta todos os desenvolvedores e profissionais de tecnologia. Precisamos garantir que nossas bases de dados de treinamento sejam amplamente representativas, não apenas em termos de etnia, mas também de gênero, idade, capacidade e muito mais. Somente assim poderemos criar soluções de IA que realmente atendam às necessidades de todos, sem reproduzir os preconceitos do passado.&lt;/p>
&lt;h2 id="o-que-podemos-fazer-para-avançar">O Que Podemos Fazer Para Avançar?
&lt;/h2>&lt;p>A tecnologia de reconhecimento de imagens tem avançado imensamente, mas a questão dos preconceitos e vieses permanece. A falta de diversidade nos dados de treinamento e nas equipes de desenvolvimento é um obstáculo que todos nós, como profissionais da área de tecnologia, precisamos superar. Como podemos garantir que os sistemas que criamos não apenas funcionem, mas também sejam justos e inclusivos? A resposta está na promoção de uma maior diversidade e inclusão em todos os aspectos do desenvolvimento tecnológico, desde a coleta de dados até a criação dos algoritmos.&lt;/p>
&lt;hr>
&lt;p>Como desenvolvedores, temos um papel crucial em moldar o futuro da tecnologia. Vamos refletir sobre como estamos criando os sistemas de IA com os quais trabalhamos. Estamos garantindo que eles sejam inclusivos e representem todos de forma justa? Se não, como podemos começar a fazer mudanças significativas? Compartilhe suas ideias e experiências, e juntos podemos trabalhar para construir um futuro mais inclusivo na tecnologia.&lt;/p>
&lt;p>&lt;em>As informações apresentadas neste post foram obtidas no ekathimerini.com, que fez referência ao artigo original do The New York Times.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Referências&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;em>BBC. (2015). Google apologises for Photos app&amp;rsquo;s racist blunder. Disponível &lt;a class="link" href="https://www.bbc.com/news/technology-33347866" target="_blank" rel="noopener"
>aqui&lt;/a>&lt;/em>&lt;/li>
&lt;li>&lt;em>Ekathimerini. (2023). Google’s photo app still can’t find gorillas. And neither can Apple’s. Disponível &lt;a class="link" href="https://www.ekathimerini.com/nytimes/1212118/googles-photo-app-still-cant-find-gorillas-and-neither-can-apples/" target="_blank" rel="noopener"
>aqui&lt;/a>&lt;/em>&lt;/li>
&lt;li>&lt;em>The New York Times. (2023). Google’s Photo App Still Can’t Find Gorillas. And Neither Can Apple’s. Disponível &lt;a class="link" href="https://www.nytimes.com/2023/05/22/technology/ai-photo-labels-google-apple.html#:~:text=Yet%20Google%2C%20whose%20Android%20software,a%20person%20as%20an%20animal" target="_blank" rel="noopener"
>aqui&lt;/a>&lt;/em>&lt;/li>
&lt;/ul></description></item></channel></rss>